\documentclass[a4paper]{article}

\usepackage[hidelinks]{hyperref}
\usepackage{amssymb}

\begin{document}

\title{FEEG3003 Interim Report $-$ Learning and Exploiting Surrogate Models for Optimising the Structure of a Design}
\author{Alexander Atack $-$ 27745449}
\date{December 2018}
\maketitle

\section{Introduction}

\subsection{Aims \& objectives}

The core aim of this project is to imagine and implement an algorithm capable of learning to solve engineering design problems.
Such an algorithm should be sufficiently general that it can be trained to operate on a range of engineering environments,
and can cope with a variety of objective functions; specifically, it must be applicable to both discrete and continuously-valued objective functions.
Artificial neural networks (ANNs) and their properties as universal function approximators will be explored as a means of achieving this goal.

For the purposes of this report, a distinction is made between "intelligent" and "na\"ive" optimisation algorithms:
an intelligent algorithm utilises some domain knowledge $-$ which may be learned $-$ to explore the solution space in a directed way,
while na\"ive optimisation algorithms explore the solution space without using any learned domain knowledge, and may be iterative in nature.

If successful, the final deliverable of this project will be a description of an algorithm meeting these criteria,
possibly including a proof of the concept on a number of toy environments.
All code used for this project can be found at \url{https://github.com/aatack/IP}.

\subsection{Overview of structure}

The project will be split into three main phases, each of which will attempt to approximate a particular function:
\begin{enumerate}
  \item Train an articial neural network, referred to as the discriminator \textbf{D}, to predict the probability that a solution satisfies some set of constraints.
  $$\mathrm{\textbf{D}} : (\mathrm{constraints}, \mathrm{solution}) \mapsto p(\mathrm{constraints \; satisfied} \; | \; \mathrm{solution})$$
  It is assumed that both the solution and set of constraints can be represented as a tensor of real values.
  \item Invert the discriminator to produce a generative model \textbf{G} parameterised by a set of constraints
  which is capable of sampling from the space of solutions which are likely to satisfy that set of constraints.
  $$\mathrm{solution} \sim \mathrm{\textbf{G}}(\mathrm{constraints}) $$
  Instead of using data to train the generator, the discriminator will be used to provide gradients for backpropagation;
  intuitively, this is similar to training the generator on the laws governing the environment rather than specific examples of solutions in that environment.
  \item Find a way to map a solution space which cannot be represented by a fixed-size tensor of real values to and from a vector representation
  so that it can be optimised using the functions learned in the previous phases.
  This will involve learning a function \textbf{L} that maps some arbitrary data structure to a $n$-dimensional latent space.
  $$\mathrm{\textbf{L}} : \mathrm{design} \mapsto \mathbb{R}^n$$
  Options for doing this include vector embeddings, recursive neural networks (RNNs), or LSTM networks.

\end{enumerate}

\section{Literature Review}

\subsection{Na\"{i}ve optimisation techniques}

A number of algorithms exist for finding the minimum of a scalar or vector function.
Many of these are gradient descent algorithms \cite{ruder17} which use information about the local gradient of the objective function
to iteratively minimise error.
Because the gradient is used to update the estimate, these algorithms often fail to converge quickly on problems where the error surface is very flat.
Algorithms such as RMSProp \cite{hinton17} and Adam \cite{kingma17} have been developed to overcome this, but still fail to converge if the objective function is discretised.

Gradient-free optimisation algorithms such as particle swarm optimisation \cite{kennedy12} solve this problem for environments of small dimensionality
by rapidly exploring the state space, then focusing on areas which give good results.
But the expected runtime of such algorithms drop off exponentially as the dimensionality of the state space increases.

Genetic algorithms \cite{carr14}, sometimes considered to fall under the umbrella of machine learning, are a certain kind of optimisation algorithm
which are both gradient-free and stochastic.
They are still not intelligent optimisation algorithms, however, because they only adapt solutions to constraints, and do not adapt their overall strategy to different environments.

\subsection{Artificial neural networks}

At times, the function being optimised will be a particular parameterisation of a specific type of function.
In this case, there may exist an analytical solution for the optimum which is a function of the parameters.
It might therefore be expected that a universal function approximator could be able to learn this mapping.

ANNs are known to be universal function approximators \cite{hornik91}.
In particular, deep neural networks (DNNs) \cite{liang17} are capable of approximating highly intractable functions to within a reasonable degree of accuracy.
One limiting factor of the performance of ANNs and DNNs is the quantity of data available.
While larger networks with more capacity are able to learn more complex mappings, they also run a higher risk of overfitting, \cite{caruana01},
a problem which is exacerbated in situations where the amount of training data may be limited.

Regularisation techniques have been developed \cite{goodfellow16, srivastava14, ioffe15, li18} to prevent the overfitting of neural networks.
These techniques generally rely on reducing the reliance of the network on singular large weights:
this makes it more difficult for the network to detect and memorise individual data points, thereby forcing it to learn a more generic mapping.
Bayesian neural networks \cite{giryes16} view the problem of finding the network weights through a Bayesian lens rather than a frequentist one.
A Bayesian network will sample its weights from a probability distribution, with weight vectors more likely to be sampled if they are
more likely to explain the dataset seen by the network during its training.
This can boost network performance when only a small number of examples are available, and can give an idea of the uncertainty
of the network by performing multiple forward passes and seeing how closely the network output from each pass agree with one another.

Adding noise to the network's input vector \cite{zur09} has also been shown to decrease overfitting and increase the generality of the network's estimates,
especially for applications where the available data are sparsely spread throughout the state space.
The effect of adding noise to the input vector is to simulate a much greater number of data points, but this method does
assume that data points which are nearby in the state space will produc similar outputs.
The level of noise is another hyperparameter which must be optimised: too little noise will have no effect on overfitting,
while too much noise will make it impossible for the network to distinguish between noise and genuine information, causing a drop in accuracy.

\subsection{Function inversion}

As well as approximating functions mapping between two sets, ANNs are capable of learning the inversion of an existing function.
Promising results have been obtained using standard ANNs to invert sometimes intractable functions in image processing,
for uses such as image deblurring \cite{nah18} and colourisation \cite{nguyen16}.

Autoencoders are an unsupervised learning method often used to pretrain layers of neural networks that will later be used for supervised learning \cite{rumelhart86},
in doing so training an encoder $f(x)$ and a decoder $f^{-1}(x)$.
This can be used to extract the more important latent features of the input space, or remove noise from corrupted data \cite{vincent08}.
When used for pretraining, the decoder is normally thrown away; but if the encoding function $f(x)$ is already know, the autoencoder architecture can be used to approximate its inverse.

Other architectures have been proposed which perform the same task as a traditional autoencoder, but train a generative model to do so.
Variational autoencoders (VAEs) are models which allow sampling from the latent space by parameterising a probability distribution over the latent space \cite{kingma14},
indicating which latent variables are most likely to explain the visible data.
The most common use for this is in learning a more robust latent mapping for a dataset, but the method could be adapted to create a generative model that inverts a many-to-one function.

A common problem faced by traditional autoencoders is that not every point in the latent space has a meaningful equivalent in the state space.
A regularisation term is imposed on a VAE in the beta-VAE architecture \cite{higgins16} that encourages a higher standard deviation in the probability distribution over the latent space.
The effect of this is that input data may be mapped to a greater variety of regions in the latent space,
and therefore a greater proportion of the latent space will have a meaningful inverse mapping.

\subsection{Adversarial algorithms}

\subsection{Representation learning}

\section{Current Progress}

\subsection{Environment design}

\subsection{Training discriminators}

\subsection{Regularisation methods}

\subsection{Hyperparameter tuning}

\section{Future Work}

\subsection{Training generators}

\subsection{Sampling from nonlinear topologies}

\begin{thebibliography}{999}

\bibitem{ruder17}
  S. Ruder,
  \emph{An overview of gradient descent optimization algorithms}.
  arXiv:1609.04747v2 [cs.LG],
  15 Jun 2017.

\bibitem{hinton17}
  G. Hinton, N. Srivastava, K. Swersky,
  \emph{Overview of mini-batch gradient descent}.
  University of Toronto
  [online lecture notes]
  2017.
  \url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}.

\bibitem{kingma17}
  D.P. Kingma, J.L. Ba,
  \emph{Adam: A Method for Stochastic Optimization}.
  arXiv:1412.6980v9 [cs.LG],
  30 Jan 2017.

\bibitem{kennedy12}
  J. Kennedy, R. Eberhart,
  \emph{Particle Swarm Optimisation}.
  Washington, DC,
  2012.

\bibitem{carr14}
  J. Carr,
  \emph{An Introduction to Genetic Algorithms}
  [online].
  May 16, 2014,
  \url{https://www.whitman.edu/Documents/Academics/Mathematics/2014/carrjk.pdf}.

\bibitem{hornik91}
  K. Hornik,
  \emph{Approximation Capabilities of Multilayer Feedforward Networks}.
  Neural Networks,
  1991,
  Volume 4,
  Issue 2,
  Pages 251-257.

\bibitem{liang17}
  S. Liang, R. Srikant,
  \emph{Why Deep Neural Networks for Function Approximation?}.
  arXiv:1610.04161v2 [cs.LG],
  3 Mar 2017.

\bibitem{caruana01}
  R. Caruana, S. Lawrence, L. Giles,
  \emph{Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping}.
  NIPS,
  2001.

\bibitem{goodfellow16}
  I. Goodfellow, Y. Bengio, A. Courville,
  2016.
  \emph{Deep Learning}
  [online],
  MIT Press.
  \url{http://www.deeplearningbook.org/contents/regularization.html}.

\bibitem{srivastava14}
  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
  \emph{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
  Journal of Machine Learning Research, 
  15 (Jun): 1929-1958, 2014.

\bibitem{ioffe15}
  S. Ioffe, C. Szegedy,
  \emph{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}.
  arXiv:1502.03167v3 [cs.LG],
  2 Mar 2015.

\bibitem{li18}
X. Li, S. Chen, X. Hu, J. Yang,
\emph{Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift}.
arXiv:1801.05134v1 [cs.LG],
16 Jan 2018.

\bibitem{giryes16}
  R. Giryes, G. Sapiro, A.M. Bronstein,
  \emph{Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?}.
  arXiv:1504.08291v5 [cs.NE],
  14 Mar 2016.

\bibitem{zur09}
  R.M. Zur, Y. Jiang, L.L. Pesce, K. Drukker,
  \emph{Noise injection for training artificial neural networks: A comparison with weight decay and early stopping}.
  Med Phys.
  2009 Oct;
  36(10): 4810–4818.

\bibitem{nah18}
  S. Nah, T.H. Kim, K.M. Lee,
  \emph{Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring}.
  arXiv:1612.02177v2 [cs.CV],
  7 May 2018.

\bibitem{nguyen16}
  T. Nguyen, K. Mori, R. Thawonmas,
  \emph{Image Colourization Using a Deep Convolutional Neural Network}.
  ASIAGRAPH Conference,
  2016.

\bibitem{rumelhart86}
  D.E. Rumelhart, G. Hinton, R.J. Williams,
  \emph{Learning internal representations by error propagation}.
  Parallel Distributed Processing,
  Vol 1: Foundations,
  MIT Press,
  Cambridge, MA,
  1986.

\bibitem{vincent08}
  P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol,
  \emph{Extracting and Composing Robust Features with Denoising Autoencoders}.
  Universit\'e de Montr\'eal,
  2008.

\bibitem{kingma14}
  D.P. Kingma, M. Welling,
  \emph{Auto-Encoding Variational Bayes}.
  arXiv:1312.6114v10 [stat.ML],
  1 May 2014.

\bibitem{higgins16}
  I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinik, S. Mohamed, A. Lerchner,
  \emph{beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework}.
  ICLR,
  04 Nov 2016.

\bibitem{anirudh18}
  R. Anirudh, J.J. Thiagarajan, B. Kailkhura, T. Bremer,
  \emph{An Unsupervised Approach to Solving Inverse Problems using Generative Adversarial Networks}.
  arXiv:1805.07281v2 [cs.CV],
  4 Jun 2018.

% Insert here

\bibitem{mikolov13}
  T. Mikolov, K. Chen, G. Corrado, J. Dean,
  \emph{Efficient Estimation of Word Representations in Vector Space}.
  arXiv:1301.3781v3 [cs.CL],
  7 Sep 2013.

\bibitem{sigmund01}
  O. Sigmund,
  \emph{A 99 line topology optimization code written in Matlab}.
  Struct Multidisc Optim 21,
  120-127,
  2001.

\bibitem{luo18}
  R. Luo, F. Tian, T. Qin, E. Chen, T. Yiu,
  \emph{Neural Architecture Optimization}.
  arXiv:1808.07233v4 [cs.LG],
  31 Oct 2018.

\bibitem{yu18}
  Y. Yu, T. Hur, J. Jung, I.G. Jang,
  \emph{Deep learning for determining a near-optimal topological design without any iteration}.
  Springer Nature,
  2018.

\end{thebibliography}

\end{document}
