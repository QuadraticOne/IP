\documentclass[a4paper]{article}

\usepackage[hidelinks]{hyperref}
\usepackage{amssymb}

\begin{document}

\title{FEEG3003 Interim Report $-$ Learning and Exploiting Surrogate Models for Optimising the Structure of a Design}
\author{Alexander Atack $-$ 27745449}
\date{December 2018}
\maketitle

\section{Introduction}

\subsection{Aims \& objectives}

The core aim of this project is to imagine and implement an algorithm capable of learning to solve engineering design problems.
Such an algorithm should be sufficiently general that it can be trained to operate on a range of engineering environments,
and can cope with a variety of objective functions; specifically, it must be applicable to both discrete and continuously-valued objective functions.
Artificial neural networks (ANNs) and their properties as universal function approximators will be explored as a means of achieving this goal.

For the purposes of this report, a distinction is made between "intelligent" and "na\"ive" optimisation algorithms:
an intelligent algorithm utilises some domain knowledge $-$ which may be learned $-$ to explore the solution space in a directed way,
while na\"ive optimisation algorithms explore the solution space without using any learned domain knowledge, and may be iterative in nature.

If successful, the final deliverable of this project will be a description of an algorithm meeting these criteria,
possibly including a proof of the concept on a number of toy environments.
All code used for this project can be found at \url{https://github.com/aatack/IP}.

\subsection{Overview of structure}

The project will be split into three main phases, each of which will attempt to approximate a particular function:
\begin{enumerate}
  \item Train an articial neural network, referred to as the discriminator \textbf{D}, to predict the probability that a solution satisfies some set of constraints.
  $$\mathrm{\textbf{D}} : (\mathrm{constraints}, \mathrm{solution}) \mapsto p(\mathrm{constraints \; satisfied} \; | \; \mathrm{solution})$$
  It is assumed that both the solution and set of constraints can be represented as a tensor of real values.
  \item Invert the discriminator to produce a generative model \textbf{G} parameterised by a set of constraints
  which is capable of sampling from the space of solutions which are likely to satisfy that set of constraints.
  $$\mathrm{solution} \sim \mathrm{\textbf{G}}(\mathrm{constraints}) $$
  Instead of using data to train the generator, the discriminator will be used to provide gradients for backpropagation;
  intuitively, this is similar to training the generator on the laws governing the environment rather than specific examples of solutions in that environment.
  \item Find a way to map a solution space which cannot be represented by a tensor of real values to and from a vector representation
  so that it can be optimised using the functions learned in the previous phases.
  This will involve learning a function \textbf{L} that maps some arbitrary data structure to a $n$-dimensional latent space.
  $$\mathrm{\textbf{L}} : \mathrm{design} \mapsto \mathbb{R}^n$$

\end{enumerate}

\section{Literature Review}

\subsection{Na\"{i}ve optimisation techniques}

\subsection{Artificial neural networks}

\subsection{Function inversion}

\subsection{Adversarial algorithms}

\subsection{Representation learning}

\section{Current Progress}

\subsection{Environment design}

\subsection{Training discriminators}

\subsection{Regularisation methods}

\subsection{Hyperparameter tuning}

\section{Future Work}

\subsection{Training generators}

\subsection{Sampling from nonlinear topologies}

\begin{thebibliography}{999}

\bibitem{hornik91}
  K. Hornik,
  \emph{Approximation Capabilities of Multilayer Feedforward Networks}.
  Neural Networks,
  1991,
  Volume 4,
  Issue 2,
  Pages 251-257.

\bibitem{anirudh18}
  R. Anirudh, J.J. Thiagarajan, B. Kailkhura, T. Bremer,
  \emph{An Unsupervised Approach to Solving Inverse Problems using Generative Adversarial Networks}.
  arXiv:1805.07281v2 [cs.CV],
  4 Jun 2018.

\bibitem{mikolov13}
  T. Mikolov, K. Chen, G. Corrado, J. Dean,
  \emph{Efficient Estimation of Word Representations in Vector Space}.
  arXiv:1301.3781v3 [cs.CL],
  7 Sep 2013.

\bibitem{giryes16}
  R. Giryes, G. Sapiro, A.M. Bronstein,
  \emph{Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?}.
  arXiv:1504.08291v5 [cs.NE],
  14 Mar 2016.

\bibitem{ioffe15}
  S. Ioffe, C. Szegedy,
  \emph{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}.
  arXiv:1502.03167v3 [cs.LG],
  2 Mar 2015.

\bibitem{srivastava14}
  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
  \emph{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
  Journal of Machine Learning Research, 
  15 (Jun): 1929-1958, 2014.

\bibitem{li18}
  X. Li, S. Chen, X. Hu, J. Yang,
  \emph{Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift}.
  arXiv:1801.05134v1 [cs.LG],
  16 Jan 2018.

\bibitem{goodfellow16}
  I. Goodfellow, Y. Bengio, A. Courville,
  2016.
  \emph{Deep Learning}
  [online],
  MIT Press.
  \url{http://www.deeplearningbook.org/contents/regularization.html}.

\bibitem{kingma14}
  D.P. Kingma, M. Welling,
  \emph{Auto-Encoding Variational Bayes}.
  arXiv:1312.6114v10 [stat.ML],
  1 May 2014.

\bibitem{higgins16}
  I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinik, S. Mohamed, A. Lerchner,
  \emph{beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework}.
  ICLR,
  04 Nov 2016.

\bibitem{sigmund01}
  O. Sigmund,
  \emph{A 99 line topology optimization code written in Matlab}.
  Struct Multidisc Optim 21,
  120-127,
  2001.

\bibitem{kingma17}
  D.P. Kingma, J.L. Ba,
  \emph{Adam: A Method for Stochastic Optimization}.
  arXiv:1412.6980v9 [cs.LG],
  30 Jan 2017.

\bibitem{luo18}
  R. Luo, F. Tian, T. Qin, E. Chen, T. Yiu,
  \emph{Neural Architecture Optimization}.
  arXiv:1808.07233v4 [cs.LG],
  31 Oct 2018.

\bibitem{yu18}
  Y. Yu, T. Hur, J. Jung, I.G. Jang,
  \emph{Deep learning for determining a near-optimal topological design without any iteration}.
  Springer Nature,
  2018.

\end{thebibliography}

\end{document}
