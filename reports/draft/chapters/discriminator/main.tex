\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Discriminator} \label{chapter:discriminator}

Previous chapters have assumed that a discriminator, $h'$, has already been trained and is readily available.
This chapter details the exact role of $h'$ and some rules of thumb for training it from data.

\section{Formal definition} \label{section:formalDefinition}

$h'(c,s)$ is a differentiable approximation of the constraint satisfaction function $h(c,s)$.
While in some environments $h$ may be known, it is assumed that most environments will be too complex for an analytical form and so it will be learned from data by an ANN.

Differentiability is required of $h'$ because gradients will pass through it during training of $g'$.
Because $h$ gives binary output, gradients will always be zero, so $h'$ is trained to output a probability that the solution satisfies the constraint, smoothing the gradients.
It is for this reason that even $h$ is known analytically, an ANN should be used to accelerate training of $g'$ by ensuring that gradients are non-zero.

\section{Training procedure} \label{section:discriminatorTrainingProcedure}

Data are provided in the form of $(c,\;s,\;h(c,s))$ tuples where the first two vectors are network inputs and the third is the label.
The discriminator will normally take the form of a standard multi-layer perceptron, taking its arguments as a concatenated vector and outputting a single scalar probability.
As such, the output layer should be sigmoid activated, but the internal activations are not constrained.

Training is accomplished by backpropagation \cite{rumelhart86}.
It is also strongly recommended that a proportion of the data (nominally, 20\%) is set aside for the purposes of validating $h'$ after training.

\section{Overfitting} \label{section:overfitting}

Because $g'$ is trained off $h'$ and not from data, ensuring that $h'$ does not overfit is essential.
A number of standard techniques should be employed, including dropout \cite{srivastava14} and L2 regularisation \cite{krogh92}.
The size of the dataset should be considered when choosing the network depth and width, and training should be stopped before validation error begins to rise (a sign that the network is overfitting).

\section{Practical considerations} \label{section:practicalConsiderations}

A number of different considerations should be taken into account on a case-by-case basis when training the discriminator.

\subsection{Incorporating environment knowledge} \label{subsection:incorporatingEnvironmentKnowledge}

Many engineering environments are understood quite well from an analytical perspective, and this knowledge can sometimes be leveraged to augment the training data.
A frequently valid assumption is that data points which are nearby in the feature space will produce similar outputs.
Adding noise to the data before feeding them into the network simulates a much greater quantity of data, thereby reducing overfitting, but may cause a loss of fidelity in environments in which the output is highly sensitive to the input.

Some environments are also very sparse: that is, only a miniscule subset of all valid environments will satisfy the constraint.
In such cases, training data may have to be hand-picked to ensure that enough positive training examples are observed.
If carrying out experiments to obtain data is also costly, a dataset of only positive examples could be gathered.
Negative samples could then be produced by taking the solution from one datum and combining it with the constraint from another datum, under the assumption that that solution will very likely fail to satisfy the constraint.

Sparse environments may also be amenable to performing dimensionality reduction on the constraints, solutions, or both, by means of an autoencoder or VAE.

\subsection{Limiting network capacity} \label{subsection:limitingNetworkCapacity}

Training a perfect discriminator is not always desirable: a perfect discriminator will have near-zero gradients by virtue of the fact that it is trying to approximate a binary function.
Deliberately forcing the discriminator to cope with greater uncertainty can reduce the chance of the discriminator's output saturating, thereby giving clearer indications to the generator by backpropagating gradients of a greater magnitude.

Adding noise to the inputs as aforementioned is one way of achieving this, with another possibility being the limitation of the capacity of the network.
Using fewer hidden nodes than would usually be used may force the discriminator to learn a simplified function, which is technically less accurate but may induce faster training of the generator.
These changes also make the discriminator less susceptible to overfitting, which is possibly a greater threat to the success of the discriminator than a slight reduction in accuracy.

\end{document}
