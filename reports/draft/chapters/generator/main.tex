\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Generator Training}

\section{Distribution matching}

\subsection{KL-divergence}

Because the use of neural networks requires a space similarity problem to be framed as a distribution similarity problem, distribution distance metrics can be used.
If the generator is modelled by a network whose p.d.f. can quickly be found, the KL-divergence between $\hat{V}'(c)$ and $\hat{V}(c)$ can be estimated by drawing samples from $\hat{V'}(c)$.
The p.d.f. of a neural network can be found by inverting it or using a constrained architecture such as a radial basis function.
Whether or not inverting a network is plausible is discussed later in this chapter.
The integral form of the KL-divergence is:
$$D_{\text{KL}}(\hat{V'}(c) \; || \; \hat{V}(c)) = 
\int_L \hat{V'}(c)(g'(l, c)) \; \text{log} \bigg(\frac
{\hat{V'}(c)(g'(l, c))}{\hat{V}(c)(g'(l, c))}
\bigg) \; dl $$
Since integration over the entirety of the latent space is unlikely to be tractable, this equation can be adapted such that the divergence is written as an expectation across a batch of samples, each weighted by their probability of occurring.
Let $\{l_1, l_2, ..., l_{j-1}, l_j\}$ be a batch of samples drawn from the latent space.
An estimation of the KL-divergence can then be calculated as:
$$\textbf{E}\big[D_{\text{KL}}(\hat{V'}(c) \; || \; \hat{V}(c))\big] =
\frac{1}{j}\sum_{i=1}^{j}
\text{log} \bigg(\frac
{\hat{V'}(c)(g'(l_i, c))}{\hat{V}(c)(g'(l_i, c))}
\bigg)
$$
The term before the logarithm disappears in the summation equation because each sample is weighted by the inverse of its probability density under $\hat{V'}(c)$.

Since all operations in this equation are differentiable, the partial derivative of the KL-divergence with respect to the weights of the generator can be found by a library with automatic differentiation such a Tensorflow.
Then the generator training is simply a matter of minimising the KL-divergence.

\subsection{Potential issues}

While this approach is theoretically very simple, there are several practical problems that could arise during training.
\begin{itemize}
    \item[] \textbf{Exploding gradients}: a problem frequently encountered when minimising functions containing a logarithm is the occurrence of NaN values resulting from taking the logarithm of zero.
    While this will not be an issue since the numerator of the fraction will be non-zero by virtue of the fact that that point has been sampled, very small values are possible.
    These will produce large negative logarithms, whose derivatives will be excessively large and positive and rapidly cause exploding gradients.
    Because it is known that large positive gradients are likely to result from a logarithm, but large negative ones are impossible, it may be possible to clip the derivatives to prevent exploding gradients while still maintaining roughly correct results.   
    \item[] \textbf{Disjoint manifolds}: the points comprising real data distributions are often described in a much higher-dimensional space than the intrinsic dimension of the distribution that created them.
    This will likely be the case for the constraint satisfaction function, and so when the learned generator is first initialised it is highly likely that $\hat{V'}(c)$ and $\hat{V}(c)$ will be disjoint.
    If this is the case then the KL-divergence will always be 0 and no training will occur.
    Methods of solving this problem, such as minimising the Wasserstein distance instead of KL-divergence, were considered, but judged to be beyond the scope of this project.
  \end{itemize}

\section{Metric proxies}

As previously discussed, modelling spaces as distributions invalidates the equations for precision and proxy that were discussed in the last chapter.
Proxy metrics will now be defined for precision and spread, which are intuitively similar to the concepts described for spaces but applicable to distributions.

\subsection{Precision}

Optimising precision is intended to ensure that as many elements from the learned viable space as possible are also in the true viable space.
When these are viewed instead as distributions, the learned viable distribution will likey cover almost the full solution space.
This does not provide the whole picture, however, as some regions of this distribution will have far higher density than others.

An adequate precision proxy might therefore be defined as one which causes a heavy penalty when samples are drawn from $\hat{V'}(c)$ which are in low-density regions of $\hat{V}(c)$.
Infrequently drawn samples from the learned viable distribution may have very high penalties, but these will not have a large impact on the overall precision proxy because they will be significantly outnumbered by large quantities of more probable (and therefore more relevant) samples.

Letting $\{l_1, l_2, ..., l_{j-1}, l_j\}$ be a batch of samples drawn from the latent space, the precision proxy can be defined as:
$$\textbf{E}\big[\hat{p}(g)\big] = -\frac{1}{j}\sum_{i=1}^{j}
\text{log} \big( 1 - h(c,g'(l, c)) + \epsilon \big)
$$
where $\epsilon$ is a small positive value added to prevent logarithms of zero.
Note that $h(c, s)$ is proportional to $\hat{V}(c)$ but is a probability instead of a probability density and so will always be between 0 and 1.

Intuitively, this is equivalent to taking a batch of points from the latent space, passing them into the solution space, calculating the satisfaction of each point, and taking the mean of the negative logarithm of each of these values.
Samples for which $\hat{V}(c)(s) \ll \hat{V'}(c)(s)$ will occur frequently (because the density of these points in $\hat{V'}(c)$ is high) but be severely penalised (because $h(c, s)$ is small and so $\text{log}(1 - h(c, s))$ will translate to a large penalty).
The result of this is that the generator function will set its weights so as to avoid that situation as much as possible.

\subsection{Recall}

In many ways, recall is analogous to the opposite of the precision.
Solutions which are highly likely to satisfy the constraint, but which occur very infrequently in the learned viable distribution, will be penalised.
$$\textbf{E}\big[\hat{r}(g)\big] = -\frac{1}{j}\sum_{i=1}^{j}
\text{log} \big( 1 - g'^{-1}(s, c) + \epsilon \big)
$$
where $\{s_1, s_2, ..., s_{j-1}, s_j\}$ is a batch of solutions drawn from $\hat{V}(c)$.
Such a batch can be produced using a Markov chain Monte Carlo method such as the Metropolis-Hastings algorithm.
While this is inefficient, it is powerful in that it samples from the true viable distribution exactly without requiring a valid mapping to that distribution from some latent distribution.
The inefficiency of sampling from this chain is not a major issue, especially given that the samples will generally be i.i.d. over a sufficiently long chain, and so can be densely sampled and then randomly reordered.

Like direct minimisation of the KL-divergence, this recall proxy assumes that $g'^{-1}$ is tractable.

\subsection{Spread}

Optimisation for the precision proxy defined above is dependent on high-density regions of $\hat{V}(c)$ being sampled at least occasionally by $\hat{V'}(c)$, and so it stands to reason that convergence will occur slowly if $\hat{V'}(c)(s) \approx 0$ for most $s$.

Training might therefore be accelerated if the generator function can be pretrained such that it samples from a wide range of the solution space at least sometimes, allowing the highest density regions of the true viable distribution to be quickly identified.
Some metrics are defined below which, when minimised, discourage the generator from sampling only a small subset of the solution space.
\begin{itemize}
    \item[] \textbf{Similarity to identity}: penalises according to the squared distance between a point in the latent space and its accompanying point in the solution space, when both spaces are linearly scaled such that they are unit hypercubes.
    This effectively forces the generator to model the identity function after its initialisation.
    $$q_\text{id}(g, l, c) = \bigg|\frac{l - a_L}{b_L - a_L} -
    \frac{g(l, c) - a_S}{b_S - a_S}\bigg|^2$$
    Since any point in the latent space can be sampled, if the generator closely approximates the identity function then all points in the solution space will be capable of being sampled and roughly equally likely to be sampled.
    One clear drawback, however, is that this spread metric requires $m = k$, which may not be desirable in high-dimensional solution spaces which are presumed to have a low intrinsic dimension.
    \item[] \textbf{Separation}: a more weakly defined spread metric for use when the dimensions of the latent and solution spaces are different.
    $q_\text{sep}$ is calculated by sampling a batch of points from $L$ and passing them through $g'$ into $S$.
    For every pair of points in the batch, the squared difference is calculated and the mean of these squared differences is taken.
    Because minimisation of $q_\text{sep}$ results in convergence to a single point, and maximisation of $q_\text{sep}$ results in a distribution over an infinite domain, a target mean separation is set as a variable hyperparameter.
    Then $|q_\text{sep} - q_\text{target}|^2$ is minimised.
    This spread metric will typically be quite slow because its calculation is of the order $\text{O}(b^2)$ as opposed to the $\text{O}(b)$ for $q_\text{id}$, where $b$ is the batch size.
  \end{itemize}
As well as being used in pretraining for efficient generator initialisation, these spread metrics could be used as a weak substitute for $\hat{r}(g)$ in the event that evaluating $\hat{V'}(c)$ is intractable.
While not strictly measuring the recall, they prevent the generator from only choosing a single point repetitively.

\section{Neural network inversion}

Some of the proposed training metrics depend on evaluating $\hat{V'}(c)$, the probability density of the learned viable distribution, at a particular point in the solution space.
If the generator function is modelled by a neural network, this is not an obviously tractable problem; this section will explore methods of solving it.

By viewing the generator as a series of transforms applied to a known distribution $-$ the latent distribution, which is known to be uniform $-$ the space of points which produce a given output can be traced back through the layers.
Integrating over this region in the latent space then dividing its volume by the volume of the latent space as a whole will yield the probability density of that particular output.

A neural network is made up of a series of layers, each of which applies three transformations to its input:
$$y = \sigma(Wx + b)$$
where $y$ is the layer output, $x$ is the layer input, $\sigma$ is the activation function, $W$ is the weight matrix, and $b$ is the bias vector.
Depending on the number of rows in $W$, the layer will expand, reduce, or maintain the dimension of the input; these three distinct cases will be examined separately.

\subsection{Bias addition}

The input which will produce a specific output after the addition of a bias can be found trivially by subtracting the bias vector from the desired output.
$$y = x + b \implies x = y - b$$
As such, the space of relevant inputs is equivalent to the space of relevant outputs translated by $-b$.

\subsection{Square weight multiplication}

A layer in which $W$ is a square matrix maintains the dimension of the output, so it is equal to the dimension of the input.
There exists exactly one input which produces the output assuming that the output is not a null vector and the determinant of the weight matrix is not zero; both these are generally valid assumptions as both conditions are highly unlikely to occur.
Therefore the relevant input vector can be found by multiplying the inverse of the weight matrix by the output vector; similarly, the relevant input space can be found by rotating the output space according to $W^{-1}$.

In general, finding the inverse of a matrix is excessively computationally expensive, especially when large matrices are used, as is often the case in neural network layers.
One special case in which this is not true is when the inverse is equal to the transpose, which is much more efficient to calculate, and occurs when the matrix is orthogonal, corresponding to a pure rotation.

Ensuring that the weight matrix is orthogonal requires the use of another property of matrices, which is that the matrix exponential of any skew-symmetric matrix is orthogonal.
Similarly, a matrix can be constructed which is guaranteed to be skew-symmetric by taking the lower triangle $Q$ of any matrix, then computing $Q - Q^T$.

In conclusion, inverting multiplication of a square weight matrix is practical as long as the weight matrix is orthogonal, which is achieved by defining the weight matrix as:
$$W = e^{\ell(W') - \ell(W')^T}$$
where $\ell$ is an operation that computer the lower triangle of a matrix and $W'$ is a base weight matrix from which the layer weights are calculated.
Backpropagating a space of relevant outputs is then simply a matter of rotating the space by $W^T = W^{-1}$.

\section{Training procedure}

This section assumes that the discriminator is either known analytically or has already been approximated by a neural network.
That process is explored in greater depth in the next chapter.

Training broadly consists of two steps: pretraining, which involves initialising the generator to explore a reasonable subset of the solution space; and training, in which the generator is trained to balance proxies for precision and recall.

\subsection{Pretraining}

A batch of samples are taken from the latent distribution and passed through the generator into the solution space.
Unless otherwise required by the specifics of the engineering environment, constraints can also be sampled uniformly from the constraint space.
Using these values, the spread error of the generator function is calculated according to either $q_\text{id}$ or $q_\text{sep}$, whichever is deemed more appropriate.

In the backward pass, the partial derivative of $q$ is calculated with respect to each generator parameter.
Gradients are applied using a gradient descent algorithm such as Adam or RMSProp.

This process is repeated until either the generator weights converge or the spread loss is sufficiently low.

\subsection{Training}

In the main training phase, latent points are sampled uniformly and constraints are sampled as in the pretraining phase.
They are matched into arbitrary pairs, each one comprising a data point.
The constraint vector is fed to both the discriminator and embedder (which in turn feeds its values to the last layer of the generator), while the latent coordinate is fed to the generator.

From these inputs, the coordinates of the generated solution with in the solution space is calculated, and the likelihood of that solution satisfying the constraint is calculated by the discriminator.
Given all these values, the precision proxy and recall proxy (whether the actual recall proxy or one of the spread metrics) can be calculated and combined into a weighted loss term.
As with the spread metric, the derivatives of this value are used to minimise the weights of the generator.
If the discriminator is modelled by a neural network, its weights remain fixed throughout this process.

\end{document}
