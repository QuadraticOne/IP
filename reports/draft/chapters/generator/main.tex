\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Parametric Generator}

\section{Distribution matching}

\subsection{KL-divergence}

Because the use of neural networks requires a space similarity problem to be framed as a distribution similarity problem, distribution distance metrics can be used.
If the generator is modelled by a network whose p.d.f. can quickly be found, the KL-divergence between $\hat{V}'(c)$ and $\hat{V}(c)$ can be estimated by drawing samples from $\hat{V'}(c)$.
The p.d.f. of a neural network can be found by inverting it or using a constrained architecture such as a radial basis function.
Whether or not inverting a network is plausible is discussed later in this chapter.
The integral form of the KL-divergence is:
$$D_{\text{KL}}(\hat{V'}(c) \; || \; \hat{V}(c)) = 
\int_L \hat{V'}(c)(g'(l, c)) \; \text{log} \bigg(\frac
{\hat{V'}(c)(g'(l, c))}{\hat{V}(c)(g'(l, c))}
\bigg) \; dl $$
Since integration over the entirety of the latent space is unlikely to be tractable, this equation can be adapted such that the divergence is written as an expectation across a batch of samples, each weighted by their probability of occurring.
Let $\{l_1, l_2, ..., l_{j-1}, l_j\}$ be a batch of samples drawn from the latent space.
An estimation of the KL-divergence can then be calculated as:
$$\textbf{E}\big[D_{\text{KL}}(\hat{V'}(c) \; || \; \hat{V}(c))\big] =
\frac{1}{j}\sum_{i=1}^{j}
\text{log} \bigg(\frac
{\hat{V'}(c)(g'(l_i, c))}{\hat{V}(c)(g'(l_i, c))}
\bigg)
$$
The term before the logarithm disappears in the summation equation because each sample is weighted by the inverse of its probability density under $\hat{V'}(c)$.

Since all operations in this equation are differentiable, the partial derivative of the KL-divergence with respect to the weights of the generator can be found by a library with automatic differentiation such a Tensorflow.
Then the generator training is simply a matter of minimising the KL-divergence.

\subsection{Potential issues}

While this approach is theoretically very simple, there are several practical problems that could arise during training.
\begin{itemize}
    \item[] \textbf{Exploding gradients}: a problem frequently encountered when minimising functions containing a logarithm is the occurrence of NaN values resulting from taking the logarithm of zero.
    While this will not be an issue since the numerator of the fraction will be non-zero by virtue of the fact that that point has been sampled, very small values are possible.
    These will produce large negative logarithms, whose derivatives will be excessively large and positive and rapidly cause exploding gradients.
    Because it is known that large positive gradients are likely to result from a logarithm, but large negative ones are impossible, it may be possible to clip the derivatives to prevent exploding gradients while still maintaining roughly correct results.   
    \item[] \textbf{Disjoint manifolds}: the points comprising real data distributions are often described in a much higher-dimensional space than the intrinsic dimension of the distribution that created them.
    This will likely be the case for the constraint satisfaction function, and so when the learned generator is first initialised it is highly likely that $\hat{V'}(c)$ and $\hat{V}(c)$ will be disjoint.
    If this is the case then the KL-divergence will always be 0 and no training will occur.
    Methods of solving this problem, such as minimising the Wasserstein distance instead of KL-divergence, were considered, but judged to be beyond the scope of this project.
  \end{itemize}

\section{Metric proxies}

As previously discussed, modelling spaces as distributions invalidates the equations for precision and proxy that were discussed in the last chapter.
Proxy metrics will now be defined for precision and spread, which are intuitively similar to the concepts described for spaces but applicable to distributions.

\subsection{Precision}

Optimising precision is intended to ensure that as many elements from the learned viable space as possible are also in the true viable space.
When these are viewed instead as distributions, the learned viable distribution will likey cover almost the full solution space.
This does not provide the whole picture, however, as some regions of this distribution will have far higher density than others.

An adequate precision proxy might therefore be defined as one which causes a heavy penalty when samples are drawn from $\hat{V'}(c)$ which are in low-density regions of $\hat{V}(c)$.
Infrequently drawn samples from the learned viable distribution may have very high penalties, but these will not have a large impact on the overall precision proxy because they will be significantly outnumbered by large quantities of more probable (and therefore more relevant) samples.

Letting $\{l_1, l_2, ..., l_{j-1}, l_j\}$ be a batch of samples drawn from the latent space, the precision proxy can be defined as:
$$\textbf{E}\big[\hat{p}(g)\big] = -\frac{1}{j}\sum_{i=1}^{j}
\text{log} \big( 1 - h(c,g'(l, c)) + \epsilon \big)
$$
where $\epsilon$ is a small positive value added to prevent logarithms of zero.
Note that $h(c, s)$ is proportional to $\hat{V}(c)$ but is a probability instead of a probability density and so will always be between 0 and 1.

Intuitively, this is equivalent to taking a batch of points from the latent space, passing them into the solution space, calculating the satisfaction of each point, and taking the mean of the negative logarithm of each of these values.
Samples for which $\hat{V}(c)(s) \ll \hat{V'}(c)(s)$ will occur frequently (because the density of these points in $\hat{V'}(c)$ is high) but be severely penalised (because $h(c, s)$ is small and so $\text{log}(1 - h(c, s))$ will translate to a large penalty).
The result of this is that the generator function will set its weights so as to avoid that situation as much as possible.

\subsection{Recall}

In many ways, recall is analogous to the opposite of the precision.
Solutions which are highly likely to satisfy the constraint, but which occur very infrequently in the learned viable distribution, will be penalised.
$$\textbf{E}\big[\hat{r}(g)\big] = -\frac{1}{j}\sum_{i=1}^{j}
\text{log} \big( 1 - g'^{-1}(s, c) + \epsilon \big)
$$
where $\{s_1, s_2, ..., s_{j-1}, s_j\}$ is a batch of solutions drawn from $\hat{V}(c)$.
Such a batch can be produced using a Markov chain Monte Carlo method such as the Metropolis-Hastings algorithm.
While this is inefficient, it is powerful in that it samples from the true viable distribution exactly without requiring a valid mapping to that distribution from some latent distribution.
The inefficiency of sampling from this chain is not a major issue, especially given that the samples will generally be i.i.d. over a sufficiently long chain, and so can be densely sampled and then randomly reordered.

Like direct minimisation of the KL-divergence, this recall proxy assumes that $g'^{-1}$ is tractable.

\subsection{Spread}

\section{Neural network inversion}

\section{Training procedure}

\end{document}
