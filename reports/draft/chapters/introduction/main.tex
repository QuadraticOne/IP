\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Introduction} \label{chapter:introduction}

\section{Motivation} \label{section:motivation}

Many engineering challenges can be expressed as optimisation problems, in which the objective is to search a space of possibilities for a value which maximises some objective function.
In real world problems, the objective function is typically complicated enough that its maximum cannot be found analytically; in these cases, it is necessary to explore the solution space in order to find the optimal solution.
A number of algorithms exist to search this space efficiently, but often converge slowly when the error surface of the objective function changes abruptly.
This will often be the case when trying to optimise continuous constraints, such as maximising the thrust of an engine, while simultaneously adhering to binary constraints, such as avoiding degenerate geometries.

Machine learning excels in exploiting underlying patterns in data to make predictions in the face of uncertainty.
This project explores the use of machine learning algorithms in constructing a differentiable mapping between an arbitrary latent space and the space of solutions to a particular set of problems such that any point in the latent space, when mapped to the solution space, will satisfy some set of binary constraints.

\section{Engineering problems} \label{section:engineeringProblems}

For the purposes of this project, an engineering problem is defined as an optimisation problem in which we must find a solution $s$ which maximises an arbitrary objective function $f(s)$, while also satisfying a given constraint $c$.
Constraint satisfaction is determined by some function $h(c,s)$ which maps a constraint and solution to one of two values, $\{\text{satisfied},\text{unsatisfied}\}$.

The functions $f$ and $h$ are deterministic and belong to an engineering environment, which can be thought of as an idealised mathematical model of a particular real-life problem.
While each environment has a unique constraint satisfaction function $h$, which is parameterised by a constraint, an environment may define a number of different continuous objective functions for which we may like to optimise.

It is also assumed that both the solution and constraint are parameterised by a vector that is bounded by an hypercube in $m$ or $n$ dimensions respectively.
The space of valid solutions is referred to as $S$, and the space of valid constraints is referred to as $C$.
$$S=\{(s_1,...,s_m)\;|\;s_i\in\{a_S,b_S\},1\le i\le m\}$$
$$C=\{(c_1,...,c_n)\;|\;c_i\in\{a_C,b_C\},1\le i\le n\}$$
where $a$ and $b$ are the arbitrary upper and lower bounds of the hypercube in each dimension.

Since both $S$ and $C$ are hypercubes, it is trivial to take samples from them.
As such, solving an engineering problem as defined here can be broken down into two steps: finding a solution vector $s$ for which $f(s)$ is suitably large; and ensuring that $h(c,s)=\text{satisfied}$ for the given constraint $c$.

\section{Learned latent mapping} \label{section:learnedLatentMapping}

Considering the two parts to solving the engineering problem, it is clear that the first part can already be achieved with conventional optimisation algorithms which have no knowledge of the objective surface.
When certain regions of the solution space do not satisfy the constraint, however, optimisation algorithms which work by exploring the solution space are not able to work efficiently since there is no natural way of reconciling a binary constraint with a scalar-valued objective function other than evaluating the constraint satisfaction function at every sampled point.
If the regions satisfying the constraint are especially sparse, it may be a considerable amount of time before even a single viable solution is found, before then having to optimise its value according to the objective function.

One way around this dilemna is to leverage the property that $h$ is constant within an environment to learn its underlying patterns using data from previous constraints that were optimised in that environment.
This knowledge can be used to create a set of viable solutions $V$, specific to the constraint, such that any solution vector which is contained within $V$ satisfies the constraint by definition:
$$V(c)=\{s\;|\;s\in S,\;h(c,s)=\text{satisfied}\}$$
While envisaging such a set is trivial, sampling from it is not.
Again, one could learn a generator function $g(c)$ which maps from an arbitrary latent space $L$ (from which samples can be easily drawn) to somewhere in $V(c)$:
$$g(l,c)\;:\;L,C\mapsto V(c)$$
$$L=\{(l_1,...,l_k)\;|\;l_i\in\{a_L,b_L\},1\le i\le k\}$$
Once $V$ and $g$ have been learned, solving the engineering problem for any particular constraint is simply a matter of performing optimisation by varying values $l\in L$ of $(f\circ g)\;(l,c)$ using any exploratory optimisation algorithm.
As such, finding a way of modelling the viable set and generator function for any given environment is the subject of this project.

\end{document}
