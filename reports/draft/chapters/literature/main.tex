\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Literature}

\section{Na\"{i}ve optimisation techniques}

A number of algorithms exist for finding the minimum of a scalar or vector function.
Many of these are gradient descent algorithms \cite{ruder17} which use information about the local gradient of the objective function
to iteratively minimise error.
Because the gradient is used to update the estimate, these algorithms often fail to converge quickly on problems where the error surface is very flat.
Algorithms such as RMSProp \cite{hinton17} and Adam \cite{kingma17} have been developed to overcome this, but still fail to converge if the objective function is discretised.

Gradient-free optimisation algorithms such as particle swarm optimisation \cite{kennedy12} solve this problem for environments of small dimensionality
by rapidly exploring the state space, then focusing on areas which give good results.
But the expected runtime of such algorithms drop off exponentially as the dimensionality of the state space increases.

Genetic algorithms \cite{carr14}, sometimes considered to fall under the umbrella of machine learning, are a certain kind of optimisation algorithm
which are both gradient-free and stochastic.
They are still not intelligent optimisation algorithms, however, because they only adapt solutions to constraints, and do not adapt their overall strategy to different environments.

\section{Artificial neural networks}

At times, the function being optimised will be a particular parameterisation of a specific type of function.
In this case, there may exist an analytical solution for the optimum which is a function of the parameters.
It might therefore be expected that a universal function approximator could be able to learn this mapping.

ANNs are known to be universal function approximators \cite{hornik91}.
In particular, deep neural networks (DNNs) \cite{liang17} are capable of approximating highly intractable functions to within a reasonable degree of accuracy.
One limiting factor of the performance of ANNs and DNNs is the quantity of data available.
While larger networks with more capacity are able to learn more complex mappings, they also run a higher risk of overfitting \cite{caruana01},
a problem which is exacerbated in situations where the amount of training data may be limited.

Regularisation techniques have been developed \cite{goodfellow16, srivastava14, ioffe15, li18} to prevent the overfitting of neural networks.
These techniques generally rely on reducing the reliance of the network on singular large weights:
this makes it more difficult for the network to detect and memorise individual data points, thereby forcing it to learn a more generic mapping.
Bayesian neural networks \cite{giryes16} view the problem of finding the network weights through a Bayesian lens rather than a frequentist one.
A Bayesian network will sample its weights from a probability distribution, with weight vectors more likely to be sampled if they are
more likely to explain the dataset seen by the network during its training.
This can boost network performance when only a small number of examples are available, and can give an idea of the uncertainty
of the network by performing multiple forward passes and seeing how closely the network outputs from each pass agree with one another.

Adding noise to the network's input vector \cite{zur09} has also been shown to decrease overfitting and increase the generality of the network's estimates,
especially for applications where the available data are sparsely spread throughout the state space.
The effect of adding noise to the input vector is to simulate a much greater number of data points, but this method does
assume that data points which are nearby in the state space will produce similar outputs.
The level of noise is another hyperparameter which must be optimised: too little noise will have no effect on overfitting,
while too much noise will make it impossible for the network to distinguish between noise and genuine information, causing a drop in accuracy.

\section{Function inversion}

As well as approximating functions mapping between two sets, ANNs are capable of learning the inversion of an existing function.
Promising results have been obtained using standard ANNs to invert sometimes intractable functions in image processing,
for uses such as image deblurring \cite{nah18} and colourisation \cite{nguyen16}.

Autoencoders are an unsupervised learning method often used to pretrain layers of neural networks that will later be used for supervised learning \cite{rumelhart86},
in doing so training an encoder $f(x)$ and a decoder $f^{-1}(x)$.
This can be used to extract the more important latent features of the input space, or remove noise from corrupted data \cite{vincent08}.
When used for pretraining, the decoder is normally thrown away; but if the encoding function $f(x)$ is already known, the autoencoder architecture can be used to approximate its inverse.

Other architectures have been proposed which perform the same task as a traditional autoencoder, but train a generative model to do so.
Variational autoencoders (VAEs) are models which allow sampling from the latent space by parameterising a probability distribution over the latent space \cite{kingma14},
indicating which latent variables are most likely to explain the visible data.
The most common use for this is in learning a more robust latent mapping for a dataset, but the method could be adapted to create a generative model that inverts a many-to-one function.

A common problem faced by traditional autoencoders is that not every point in the latent space has a meaningful equivalent in the state space.
A regularisation term is imposed on a VAE in the $\beta$-VAE architecture \cite{higgins16} that encourages a higher standard deviation in the probability distribution over the latent space.
The effect of this is that input data may be mapped to a greater variety of regions in the latent space,
and therefore a greater proportion of the latent space will have a meaningful inverse mapping.

\section{Adversarial algorithms}

More recently, algorithms have been designed to train generative models using two competing networks in a minimax game.
The most common adversarial networks, generative adversarial networks (GANs), can be trained to sample from an arbitrary distribution \cite{goodfellow14, horger18}.
These have been successfully applied to a range of problems, including image resolution upscaling \cite{ledig17} and creating cross-domain transfer functions \cite{zhu18}.
Such applications could be useful in using GANs to sample from a space of viable solutions, if the space can be parameterised according to the relevant constraints.

Some research has also been done into the use of GANs to invert functions \cite{anirudh18};
while autoencoders can also accomplish this task and are generally considered to be easier to train \cite{bang18},
the generative nature of GANs is appealing considering that any given engineering problem may have more than one solution.

\section{Representation learning}

All of the neural network architectures discussed so far require the input data to be presented in the form of a fixed-size tensor of real values.
While many practical data structures can be meaningfully encoded in this form, many engineering problems can have solutions which are discrete,
unbounded, or otherwise not easily represented by a tensor of real values.

Long short-term memory networks (LSTMs) \cite{hochreiter97} are stateful networks which collate information over a linear series of tensors into a fixed-size memory.
At each time step, another item from the series is viewed by the network, and it decides which information to keep, and which to forget.
If trained properly, the memory tensor at the end is a fixed-size representation of the important features of the list.

Recursive neural networks merge information about pairs of objects, allowing the construction of representations of tree-like data structures \cite{socher11}.
As such, they are frequently used to parse sentences in natural language processing, though this technique can be applied to other trees as well.
Like with LSTMs, because the information capacity of the final state tensor is finite, the network is forced to either generalise, or forget unimportant information.

Some data structures, known as tokens, cannot be represented by real values and do not consist of sub-elements that can.
The prime example of these data structures are words; each word carries meaning but the meaning of each word is encoded in the understanding of the people using them.
Vector space models represent tokens as points in a vector space, and some methods have been developed for embedding tokens in this vector space.
One method commonly used is \emph{word2vec} \cite{mikolov13}, which embeds words into a dense vector space, though the same principle
has been adapted for use on tokens other than words \cite{le14}.

\end{document}
