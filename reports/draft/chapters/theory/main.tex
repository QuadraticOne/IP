\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Theoretical Formulation}

\section{Solution and constraint spaces}

As stated in the introduction, the objective of this project is to find a way to learn a space of viable solutions parameterised by a constraint $V(c)$, and an efficient method of taking samples from it.
Let $g'(l, c)$ be the learned generator function as opposed to the true generator function.
Then let $V'(c)$ be the learned viable space: the space of all solutions which can be obtained by passing a coordinate in the latent space through the generator:
$$V'(c) = \{s \; | \; g'^{-1}(s, c) \in L\}$$
If we assume that $g$ is modelled by an artificial neural network then the condition that samples from $V(c)$ can be drawn efficiently is satisfied when $V(c) \approx V'(c)$, simply by sampling a point from $L$ and passing it through $g$.

It is therefore necessary to define a distance metric for two spaces, which the neural network can be trained to minimise.
Defining such a metric for two general vector space is not obvious, especially considering that the capacity of the neural network is limited in practice (even if not in theory) and so the two spaces will never be exactly equal.

The performance of the generator function will therefore be measured in two dimensions, each describing a desired trait of the learned viable space:
\begin{itemize}
    \item[] \textbf{Precision}: the proportion of samples from the learned generator function which are actually members of the true viable space.
    $$p(g) = \int_{C}\int_{L} \; 1_{g(l, c) \; \in \; V(c)} \; dl \; dc$$ 
    \item[] \textbf{Recall}: the proportion of solutions which actually satisfy the constraint which are members of the learned viable space.
    $$r(g) = \int_{C}\int_{V(c)} 1_{g'^{-1}(s, c) \; \in \; L} \; ds \; dc$$ 
\end{itemize}
Methods for estimating these quantities will be discussed in Chapter 4.

Given that the network capacity is limited, a fundamental tradeoff exists between the precision and recall of the generator function.
Too heavily weighting precision will result in a generator function which picks from a small selection of very promising solutions but misses many potential solutions; prioritising recall will result in a much larger range of solutions but which are far less likely to satisfy the constraint.

This tradeoff is parameterised by a hyperparameter $w$ describing the weight of the recall in the loss function $\mathcal{L}$ that the neural network is trained to minimise.
The value of this weight can be tuned to determine how confident the network should be that its suggestions will satisfy the constraint versus how confident it is that it is sampling from all the possible solutions.
$$\mathcal{L}(g) = p(g) + w \cdot r(g)$$

\section{Modelling spaces as distributions}

One limitation of neural networks for this application is the fact that their output space is always unimodal (given that the input space is unimodal), while the viable space for a constraint will almost always be multimodal.
This arises as a consequence of the differentiability of neural networks, which is a desirable quality for this application so that gradient-based optimisation algorithms can still be used in the latent space.
A consequence of this is that the precision and recall, as defined above, will carry very little meaning because almost all solution vectors will be part of the learned viable space, even though small distances in the latent space can be mapped to very large distances in the solution space (effectively changing the density of solutions sampled from the generator function).

This problem is solved by modelling each space as a probability distribution and defining proxy measures for precision and recall.
Whether or not a point belongs to a space is no longer a deterministic binary property, but is instead a fuzzy value interpreted as the density of the distribution at that point.
If the density at point $A$ is much greater than the density at point $B$, we can say that it is far more likely that point $A$ belongs to the space than point $B$.
The probability distribution function representing a space $X$ is denoted by $\hat{X}$.

Arbitrarily, the latent space is defined to be a uniform distribution within its bounds, which are set to 0 and 1 for the sake of simplicity.
$$\hat{L}(l) =
\left\{
	\begin{array}{ll}
		1 & \mbox{if } 0 \le l_i \le 1 \; \forall \; \{i \; | \; 1 \le i \le k\} \\
		0 & \mbox{otherwise}
	\end{array}
\right.$$
Since the learned viable space is formed by passing the latent space through the learned generator, its p.d.f. can be defined as the integral of the latent space p.d.f. over all points in the latent space which map to that solution:
$$\hat{V'}(c)(s) = \int_{\{l \; | \; g(l,c) = s\}} \hat{L}(l) \; dl$$
Finally, the true viable space's p.d.f. is defined for any solution, and is $0$ if the solution does not satisfy the constraint, and $\gamma$ if it does.
The value of $\gamma$ is chosen such that the total mass of the p.d.f. is unity.
$$\hat{V}(c)(s) =
\left\{
	\begin{array}{ll}
		\gamma & \mbox{if } h(c,s) = \text{satisfied} \\
		0 & \mbox{otherwise}
	\end{array}
\right.$$
The continuity of neural networks should therefore not pose an issue to the feasibility of this approach to solving the problem at hand, because while regions of the solution space which are not part of the viable space may be sampled occasionally, if the generator is trained well then these regions could be much less dense than regions that are highly likely to satisfy the constraint.
Intuitively, this can be thought of as the mapping pulling together the distinct modes of the viable space, reducing the distance between them and thereby making it easier for optimisation algorithms to find a global maximum by exploration.
$$\text{TODO: add graphic displaying this process}$$
Proxy metrics can now be defined which are intuitively similar to the precision and recall but are functions of a p.d.f. rather than of the generator function itself.
It is also possible to consider a distance metric between two distributions $-$ which is much easier to define than a distance metric between two spaces $-$ and minimise this directly, to prompt $\hat{V'}(c)$ to match $\hat{V}(c)$.
These avenues are both explored in the next chapter.

\section{Adapting the GAN architecture}

A probabilistic interpretation of vector spaces has been introduced which provides information on the density of points within a generated space, thereby working around the limitation of neural networks that prevents them from making discontinuous jumps from one mode to another.
This will also later allow the definition of more tractable loss functions that can be used to train the learned generator function.
The network architecture, however, is still not clear.
$$\text{TODO: diagram of a typical GAN architecture}$$
GANs, introduced in \S 2.4, are a family of neural network architectures which allow samples to be drawn from a distribution.
A typical GAN architecture has a generator, taking samples from the latent space to the feature space, and a discriminator, which determines whether points in the feature space are true or generated.
Their ability to sample from a distribution is already very close to the required functionality of the learned generator, so some adaptations to the traditional GAN will be proposed.

\subsection{Pretrained discriminator}

Traditional GAN training procedures update both the discriminator and generator iteratively.
This forces the generator to learn a range of outputs $-$ effectively punishing low recall $-$ by having the discriminator adapt continuously to the changing output of the generator.

In an engineering problem, however, $h$ is fixed and does not change.
Whether $h$ is known in advance or learned from data, it does not make sense for it to be updated alongside the generator since the constraint satisfaction function (discriminator) is immutable.
This has already been accounted for by optimising for a combination of precision and recall, forcing the generator to not overspecialise.

\subsection{Curried generator}

Distributions learned by GANs are normally fixed.
The distribution $\hat{V}(c)$ is parameterised by the constraint vector, however, and so any adapted GAN architecture needs to be capable of taking a constraint as input and adapting its output accordingly.

Perhaps a more natural way of thinking about this is to consider $g$ as a curried function; that is, it takes a constraint and then returns another function which maps from the latent space to the solution space.
This is a subtle semantic difference to a dyadic function, but may provide direction as to the best way of integrating the constraint vector into the GAN architecture.

Consider a neural network designed to perform multi-class classification.
When fed a feature vector, the activations of units in the hidden layers do not depend on the class; in fact, information about the class is only added in the output layer.
Likelihood estimations for each class are produced by multiplying the weights matrix of the final layer by the activation vector of the final hidden layer, and are as such independent of each other until a softmax activation is applied.
As such, the likelihood of each class can be viewed as the dot product of the final hidden layer's activations with some vector embedding of the class.

$Q$-learning performed by Deep $Q$ Networks on a discrete action space perform a similar action, except in a regression setting rather than a classification setting.
Crucially: the DQN is a function of two parameters (state and action) which has been curried to take only one parameter (state) with the other built into its weights; the quality values produced for each action are independent of each other; the resulting quality values represent an unbounded scalar instead of a probability, and; each quality value is dependent only on the dot product of the final hidden layer's activations and some vector embedding of the related action.

In other words, DQNs prove that it is possible to use neural networks as curried functions where the weights of the last layer only are provided as an embedding of the first parameter.

To adapt this to our use case, we need only realise that a constraint embedding can be produced as a function of the constraint vector, which could feasibly be approximated by another neural network.
So information about the constraint can be passed into the generator network by estimating the weights of its output layer using a separate network, the embedder network, which takes only the constraint as input.

\subsection{Data-free training}

If the discriminator has already been trained, or is known, no data are needed to train the generator because samples from it can be used as training examples.
Exactly how this works will be discussed in the next chapter.
Hence, training data are only used as examples for training the discriminator, and are not needed to train the generator.
The advantags to this approach are twofold: examples are only needed to train a normal neural network individually rather than adversarial networks in a GAN (which are notoriously hard to train), and the generator is only capable of overfitting to the extent that the discriminator has overfit its data.

When the constraint satisfaction is known already, any overfitting can be combatted by increasing the generator capacity or training for longer.
When the constraint satisfaction function is modelled by a discriminator learned from data, knowledge of the engineering environment can be used to augment data to alleviate overfitting.
In any case, moving all responsibility for overfitting from the generator to the discriminator $-$ where it can be more easily measured and avoided $-$ is an exceptional advantage to adapting the GAN architecture in this way.

\end{document}
